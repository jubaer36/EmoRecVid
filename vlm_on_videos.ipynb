{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6434ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for Tarsier2-Recap-7b (Qwen2-VL backbone)\n",
    "# %pip install -q qwen-vl-utils\n",
    "\n",
    "# Install missing dependencies for FaceScanPaliGemma_Emotion\n",
    "# %pip install -q transformers>=4.42.0 Pillow opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a95221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 02:08:35.258835: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-03-01 02:08:35.474770: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "VRAM: 12.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MANIFEST_PATH = BASE_DIR / \"utterance_clips\" / \"manifest.csv\"\n",
    "\n",
    "print(\"Imports OK\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137e2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: KlingTeam/VidEmo-3B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ade660a678d49569cfea7caf6822802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_ID = \"KlingTeam/VidEmo-3B\"\n",
    "PROCESSOR_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"   # base model that ships the processor config\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",   # remove if flash-attn not installed\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(PROCESSOR_ID)\n",
    "# Decoder-only models require left-padding for correct batched generation\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37ad47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_clip() ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROMPT_TEXT = (\n",
    "    \"Watch this short video clip of a person speaking and describe how their emotional state evolves over time.\\n\"\n",
    "    \"Structure your response as a temporal progression — divide the clip into beginning, middle, and end (or more segments if the emotion changes more than once).\\n\"\n",
    "    \"For each segment describe:\\n\"\n",
    "    \"- **Facial Expressions**: Specific muscle movements (brow, lips, eyes, jaw, cheeks).\\n\"\n",
    "    \"- **Head & Gaze**: Tilts, nods, shakes, eye direction.\\n\"\n",
    "    \"- **Body Language**: Posture shifts, gestures, tension or relaxation.\\n\"\n",
    "    \"- **Emotion at this moment**: The most likely emotion and the visual cues supporting it.\\n\"\n",
    "    \"Finish with a one-sentence summary of the overall emotional arc (e.g., starts neutral → builds frustration → brief smile at end).\\n\"\n",
    "    \"Ground every observation in a specific visible signal.\"\n",
    ")\n",
    "\n",
    "\n",
    "def analyze_clip(video_path: str, max_new_tokens: int = 512, fps: float = 2.0) -> str:\n",
    "    \"\"\"\n",
    "    Run VidEmo-3B on a single utterance video clip and return a temporal\n",
    "    emotional arc analysis (how expressions change from start to end).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": fps,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"analyze_clip() ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64347f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load manifest – preserve any already-completed analyses\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "manifest[\"vlm_analysis\"] = manifest[\"vlm_analysis\"].astype(\"object\")\n",
    "# Create column if it doesn't exist\n",
    "if \"vlm_analysis\" not in manifest.columns:\n",
    "    manifest[\"vlm_analysis\"] = pd.NA\n",
    "manifest['vlm_analysis'] = None\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb63fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips : 10039\n",
      "Already done: 0  |  Remaining: 10039\n",
      "Queued for processing: 10039 clips\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab85169a38b47cea9936fa8e5158aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VLM analysis:   0%|          | 0/10039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using torchvision to read video.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m video_paths = [\u001b[38;5;28mstr\u001b[39m(BASE_DIR / row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m batch_rows.iterrows()]\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     results = \u001b[43manalyze_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, analysis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_idx, results):\n\u001b[32m     93\u001b[39m         manifest.at[idx, \u001b[33m\"\u001b[39m\u001b[33mvlm_analysis\u001b[39m\u001b[33m\"\u001b[39m] = analysis\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36manalyze_batch\u001b[39m\u001b[34m(video_paths, max_new_tokens, fps)\u001b[39m\n\u001b[32m     51\u001b[39m inputs = processor(\n\u001b[32m     52\u001b[39m     text=texts,\n\u001b[32m     53\u001b[39m     images=image_inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     **video_kwargs,\n\u001b[32m     58\u001b[39m ).to(model.device)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m generated_ids_trimmed = [\n\u001b[32m     68\u001b[39m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids):]\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)\n\u001b[32m     70\u001b[39m ]\n\u001b[32m     71\u001b[39m responses = processor.batch_decode(\n\u001b[32m     72\u001b[39m     generated_ids_trimmed, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     73\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total  = len(manifest)\n",
    "done_before = manifest[\"vlm_analysis\"].notna().sum()\n",
    "print(f\"Total clips : {total}\")\n",
    "print(f\"Already done: {done_before}  |  Remaining: {total - done_before}\")\n",
    "\n",
    "# ── Config ────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE = 8      # increase for throughput (reduce if OOM)\n",
    "SAVE_EVERY = 20     # checkpoint to disk every N clips\n",
    "FPS        = 1.0    # higher fps = more temporal resolution for tracking changes\n",
    "\n",
    "\n",
    "def analyze_batch(video_paths: list[str], max_new_tokens: int = 512, fps: float = FPS) -> list[str]:\n",
    "    \"\"\"Run the VLM on a batch of video clips and return a temporal arc analysis per clip.\"\"\"\n",
    "    PROMPT_TEXT = (\n",
    "        \"Watch this short video clip of a person speaking and describe how their emotional state evolves over time.\\n\"\n",
    "        \"Structure your response as a temporal progression — divide the clip into beginning, middle, and end (or more segments if the emotion changes more than once).\\n\"\n",
    "        \"For each segment describe:\\n\"\n",
    "        \"- **Facial Expressions**: Specific muscle movements (brow, lips, eyes, jaw, cheeks).\\n\"\n",
    "        \"- **Head & Gaze**: Tilts, nods, shakes, eye direction.\\n\"\n",
    "        \"- **Body Language**: Posture shifts, gestures, tension or relaxation.\\n\"\n",
    "        \"- **Emotion at this moment**: The most likely emotion and the visual cues supporting it.\\n\"\n",
    "        \"Finish with a one-sentence summary of the overall emotional arc (e.g., starts neutral → builds frustration → brief smile at end).\\n\"\n",
    "        \"Ground every observation in a specific visible signal.\"\n",
    "    )\n",
    "\n",
    "    all_messages = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": vp, \"fps\": fps, \"max_pixels\": 360 * 420},\n",
    "                    {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for vp in video_paths\n",
    "    ]\n",
    "\n",
    "    texts = [\n",
    "        processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        for msgs in all_messages\n",
    "    ]\n",
    "\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "        all_messages, return_video_kwargs=True\n",
    "    )\n",
    "\n",
    "    if \"fps\" in video_kwargs and isinstance(video_kwargs[\"fps\"], list):\n",
    "        del video_kwargs[\"fps\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    responses = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return [r.strip() for r in responses]\n",
    "\n",
    "\n",
    "# ── Main loop ─────────────────────────────────────────────────────────\n",
    "pending = list(manifest[manifest[\"vlm_analysis\"].isna()].index)\n",
    "print(f\"Queued for processing: {len(pending)} clips\\n\")\n",
    "\n",
    "processed = 0\n",
    "\n",
    "with tqdm(total=len(pending), desc=\"VLM analysis\") as pbar:\n",
    "    for batch_start in range(0, len(pending), BATCH_SIZE):\n",
    "        batch_idx = pending[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "        video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "        try:\n",
    "            results = analyze_batch(video_paths)\n",
    "            for idx, analysis in zip(batch_idx, results):\n",
    "                manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "        except Exception as e:\n",
    "            for idx in batch_idx:\n",
    "                manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "            print(f\"\\nError on batch {batch_idx}: {e}\")\n",
    "\n",
    "        prev_processed = processed\n",
    "        processed += len(batch_idx)\n",
    "        pbar.update(len(batch_idx))\n",
    "\n",
    "        if processed // SAVE_EVERY > prev_processed // SAVE_EVERY:\n",
    "            manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final save\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "done   = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nDone. {done}/{total} clips processed, {errors} errors.\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "manifest.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b965e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Pass 0 ── Empty: 10015  |  Errored: 0  |  Bad: 0  |  Total to fix: 10015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b500a1e44945b9aa9873086c47c944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retry pass 1:   0%|          | 0/10015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m video_paths = [\u001b[38;5;28mstr\u001b[39m(BASE_DIR / row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m batch_rows.iterrows()]\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     results = \u001b[43manalyze_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, analysis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_idx, results):\n\u001b[32m     51\u001b[39m         manifest.at[idx, \u001b[33m\"\u001b[39m\u001b[33mvlm_analysis\u001b[39m\u001b[33m\"\u001b[39m] = analysis\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36manalyze_batch\u001b[39m\u001b[34m(video_paths, max_new_tokens, fps)\u001b[39m\n\u001b[32m     51\u001b[39m inputs = processor(\n\u001b[32m     52\u001b[39m     text=texts,\n\u001b[32m     53\u001b[39m     images=image_inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     **video_kwargs,\n\u001b[32m     58\u001b[39m ).to(model.device)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m generated_ids_trimmed = [\n\u001b[32m     68\u001b[39m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids):]\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)\n\u001b[32m     70\u001b[39m ]\n\u001b[32m     71\u001b[39m responses = processor.batch_decode(\n\u001b[32m     72\u001b[39m     generated_ids_trimmed, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     73\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ── Retry empty / errored / bad-output clips (loops until all are clean) ──\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "def _is_bad_output(val) -> bool:\n",
    "    \"\"\"True if a non-error value looks like a corrupted/truncated generation.\"\"\"\n",
    "    if pd.isna(val) or str(val).startswith(\"ERROR:\"):\n",
    "        return False\n",
    "    s = str(val).strip()\n",
    "    words = s.split()\n",
    "    if len(words) <= 10:\n",
    "        return True\n",
    "    if words[0].lower() == \"the\" and words[-1].lower() == \"the\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_retry_mask(df):\n",
    "    is_error = df[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False)\n",
    "    is_empty = df[\"vlm_analysis\"].isna()\n",
    "    is_bad   = df[\"vlm_analysis\"].map(_is_bad_output)\n",
    "    return is_error | is_empty | is_bad\n",
    "\n",
    "pass_num = 0\n",
    "\n",
    "while True:\n",
    "    retry_mask = get_retry_mask(manifest)\n",
    "    retry_idx  = list(manifest[retry_mask].index)\n",
    "\n",
    "    is_error = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False)\n",
    "    is_empty = manifest[\"vlm_analysis\"].isna()\n",
    "    is_bad   = manifest[\"vlm_analysis\"].map(_is_bad_output)\n",
    "\n",
    "    print(f\"\\n── Pass {pass_num} ── Empty: {is_empty.sum()}  |  Errored: {is_error.sum()}  |  Bad: {is_bad.sum()}  |  Total to fix: {len(retry_idx)}\")\n",
    "\n",
    "    if len(retry_idx) == 0:\n",
    "        print(\"All clips are clean — done!\")\n",
    "        break\n",
    "\n",
    "    pass_num += 1\n",
    "    processed = 0\n",
    "\n",
    "    with tqdm(total=len(retry_idx), desc=f\"Retry pass {pass_num}\") as pbar:\n",
    "        for batch_start in range(0, len(retry_idx), BATCH_SIZE):\n",
    "            batch_idx  = retry_idx[batch_start : batch_start + BATCH_SIZE]\n",
    "            batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "            video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "            try:\n",
    "                results = analyze_batch(video_paths)\n",
    "                for idx, analysis in zip(batch_idx, results):\n",
    "                    manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "            except Exception as e:\n",
    "                for idx in batch_idx:\n",
    "                    manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "                print(f\"\\nError on batch {list(batch_idx)}: {e}\")\n",
    "\n",
    "            prev_processed = processed\n",
    "            processed += len(batch_idx)\n",
    "            pbar.update(len(batch_idx))\n",
    "\n",
    "            if processed // SAVE_EVERY > prev_processed // SAVE_EVERY:\n",
    "                manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "    manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final summary\n",
    "total  = len(manifest)\n",
    "done   = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nFinal: {done}/{total} clips processed, {errors} errors, {pass_num} retry pass(es).\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "manifest.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4840d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_clip(\"utterance_clips/Session1/Ses01F_impro06/Ses01F_impro06_F006.avi\", max_new_tokens=512, fps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── FaceScanPaliGemma_Emotion ─────────────────────────────────────────\n",
    "# # Fine-tuned PaliGemma-3B on AffectNet; classifies a face image into one of:\n",
    "# #   neutral | happy | sad | surprise | fear | disgust | anger | contempt\n",
    "# # Model card: https://huggingface.co/NYUAD-ComNets/FaceScanPaliGemma_Emotion\n",
    "\n",
    "# import os\n",
    "# # Disable the xet/CAS backend so HF falls back to standard HTTP downloads\n",
    "# os.environ[\"HF_HUB_DISABLE_XET_BACKEND\"] = \"1\"\n",
    "\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# from tqdm.auto import tqdm\n",
    "# from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "# from collections import Counter\n",
    "\n",
    "# BASE_DIR = Path(\"/mnt/Work/ML/Code/EmoRecVid\")\n",
    "# MANIFEST_PATH = BASE_DIR / \"utterance_clips\" / \"manifest.csv\"\n",
    "\n",
    "# PG_MODEL_ID = \"NYUAD-ComNets/FaceScanPaliGemma_Emotion\"\n",
    "# PG_PROC_ID  = \"google/paligemma-3b-pt-224\"\n",
    "# PG_PROMPT   = \"what is the emotion of the person in the image?\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Device: {device}\")\n",
    "\n",
    "# print(f\"Loading processor from {PG_PROC_ID} …\")\n",
    "# pg_processor = PaliGemmaProcessor.from_pretrained(PG_PROC_ID)\n",
    "\n",
    "# print(f\"Loading model from {PG_MODEL_ID} …\")\n",
    "# pg_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "#     PG_MODEL_ID,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# pg_model.to(device)\n",
    "# pg_model.eval()\n",
    "# print(\"PaliGemma emotion model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VALID_EMOTIONS = {\"neutral\", \"happy\", \"sad\", \"surprise\", \"fear\", \"disgust\", \"anger\", \"contempt\"}\n",
    "\n",
    "# def extract_frames(video_path: str, n_frames: int = 5) -> list[Image.Image]:\n",
    "#     \"\"\"Extract `n_frames` evenly-spaced frames from a video file as PIL images.\"\"\"\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     if total == 0:\n",
    "#         cap.release()\n",
    "#         return []\n",
    "#     indices = np.linspace(0, total - 1, min(n_frames, total), dtype=int)\n",
    "#     frames = []\n",
    "#     for idx in indices:\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "#         ret, frame = cap.read()\n",
    "#         if ret:\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frames.append(Image.fromarray(frame_rgb))\n",
    "#     cap.release()\n",
    "#     return frames\n",
    "\n",
    "\n",
    "# # PaliGemma requires the <image> token at the start of the prompt\n",
    "# _PG_PROMPT_WITH_TOKEN = \"<image> \" + PG_PROMPT\n",
    "\n",
    "# def predict_emotion_video(video_path: str, n_frames: int = 5) -> str:\n",
    "#     \"\"\"\n",
    "#     Extract frames, run PaliGemma on each frame, and return the per-frame\n",
    "#     emotion labels as a space-separated string, e.g.:\n",
    "#         \"neutral happy neutral sad neutral\"\n",
    "#     Labels that fall outside VALID_EMOTIONS are included as-is so no\n",
    "#     information is lost.\n",
    "#     Returns \"ERROR: no frames\" if the video has no readable frames.\n",
    "#     \"\"\"\n",
    "#     frames = extract_frames(video_path, n_frames=n_frames)\n",
    "#     if not frames:\n",
    "#         return \"ERROR: no frames\"\n",
    "\n",
    "#     per_frame_labels = []\n",
    "\n",
    "#     for frame in frames:\n",
    "#         inputs = pg_processor(\n",
    "#             text=_PG_PROMPT_WITH_TOKEN,\n",
    "#             images=frame,\n",
    "#             padding=\"longest\",\n",
    "#             do_convert_rgb=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(device)\n",
    "#         inputs = inputs.to(dtype=pg_model.dtype)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = pg_model.generate(**inputs, max_length=500)\n",
    "\n",
    "#         # Trim input tokens, decode only newly generated tokens\n",
    "#         new_tokens = output[0][inputs.input_ids.shape[1]:]\n",
    "#         decoded = pg_processor.decode(new_tokens, skip_special_tokens=True).strip().lower()\n",
    "\n",
    "#         # First word as the label candidate\n",
    "#         label = decoded.split()[0] if decoded.split() else \"unknown\"\n",
    "#         per_frame_labels.append(label)\n",
    "\n",
    "#     return \" \".join(per_frame_labels)\n",
    "\n",
    "\n",
    "# print(\"predict_emotion_video() ready  (returns space-separated per-frame labels)\")\n",
    "# print(f\"Prompt: {_PG_PROMPT_WITH_TOKEN!r}\")\n",
    "# print(f\"Extracts 5 frames per clip – one label per frame, all stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Quick sanity-check: sample a random clip and visualise predictions ─\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display\n",
    "\n",
    "# manifest_preview = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# # Pick a random row\n",
    "# sample_row = manifest_preview.sample(1).iloc[0]\n",
    "# sample_path = str(BASE_DIR / sample_row[\"path\"])\n",
    "\n",
    "# print(f\"Utterance : {sample_row['utterance_id']}\")\n",
    "# print(f\"GT emotion: {sample_row['emotion']}\")\n",
    "# print(f\"Video path: {sample_path}\")\n",
    "# print()\n",
    "\n",
    "# # Run through predict_emotion_video – returns \"label1 label2 label3 ...\"\n",
    "# result_str = predict_emotion_video(sample_path, n_frames=5)\n",
    "# per_frame_labels = result_str.split()\n",
    "# valid = [l for l in per_frame_labels if l in VALID_EMOTIONS]\n",
    "\n",
    "# print(f\"Per-frame labels : {per_frame_labels}\")\n",
    "# print(f\"Valid labels     : {valid}\")\n",
    "# print(f\"Dominant         : {Counter(valid).most_common(1)[0][0] if valid else 'none'}\")\n",
    "\n",
    "# # Plot frames with their labels\n",
    "# frames = extract_frames(sample_path, n_frames=5)\n",
    "# n = len(frames)\n",
    "# fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "# if n == 1:\n",
    "#     axes = [axes]\n",
    "\n",
    "# for ax, frame, lbl in zip(axes, frames, per_frame_labels):\n",
    "#     ax.imshow(frame)\n",
    "#     color = \"green\" if lbl in VALID_EMOTIONS else \"red\"\n",
    "#     ax.set_title(lbl, fontsize=13, color=color, fontweight=\"bold\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# dominant = Counter(valid).most_common(1)[0][0] if valid else \"none\"\n",
    "# fig.suptitle(\n",
    "#     f\"GT: {sample_row['emotion']}   |   Labels: {' '.join(per_frame_labels)}   |   Dominant: {dominant}\",\n",
    "#     fontsize=12, fontweight=\"bold\", y=1.02\n",
    "# )\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Main inference loop ───────────────────────────────────────────────\n",
    "# manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# # Ensure the column exists\n",
    "# if \"paligemma_emotion\" not in manifest.columns:\n",
    "#     manifest[\"paligemma_emotion\"] = pd.NA\n",
    "\n",
    "# # Reset all existing predictions so everything gets reprocessed\n",
    "# manifest[\"paligemma_emotion\"] = pd.NA\n",
    "\n",
    "# total   = len(manifest)\n",
    "# pending = list(manifest.index)\n",
    "\n",
    "# print(f\"Total clips to process: {total}\")\n",
    "# print(\"Output format: space-separated per-frame labels, e.g. 'neutral happy neutral sad neutral'\")\n",
    "\n",
    "# PG_N_FRAMES   = 5     # frames sampled per clip\n",
    "# PG_SAVE_EVERY = 50    # checkpoint frequency\n",
    "\n",
    "# processed = 0\n",
    "\n",
    "# with tqdm(total=len(pending), desc=\"PaliGemma emotion\") as pbar:\n",
    "#     for idx in pending:\n",
    "#         row = manifest.loc[idx]\n",
    "#         video_path = str(BASE_DIR / row[\"path\"])\n",
    "\n",
    "#         try:\n",
    "#             result = predict_emotion_video(video_path, n_frames=PG_N_FRAMES)\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = result\n",
    "#         except Exception as e:\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = f\"ERROR: {e}\"\n",
    "#             print(f\"\\nError on {row['utterance_id']}: {e}\")\n",
    "\n",
    "#         processed += 1\n",
    "#         pbar.update(1)\n",
    "\n",
    "#         if processed % PG_SAVE_EVERY == 0:\n",
    "#             manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# # Final save\n",
    "# manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# done   = manifest[\"paligemma_emotion\"].notna().sum()\n",
    "# errors = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "# print(f\"\\nDone. {done}/{total} clips processed, {errors} errors.\")\n",
    "# print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "# manifest[[\"utterance_id\", \"emotion\", \"paligemma_emotion\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ce973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Retry errored / empty / no-valid-emotion PaliGemma predictions ────\n",
    "# manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# def _has_no_valid_emotion(val) -> bool:\n",
    "#     \"\"\"True if val is a non-error string but contains no recognised emotion word.\"\"\"\n",
    "#     if pd.isna(val) or str(val).startswith(\"ERROR:\"):\n",
    "#         return False\n",
    "#     return not any(w in VALID_EMOTIONS for w in str(val).split())\n",
    "\n",
    "# is_error      = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False)\n",
    "# is_empty      = manifest[\"paligemma_emotion\"].isna()\n",
    "# is_no_emotion = manifest[\"paligemma_emotion\"].map(_has_no_valid_emotion)\n",
    "# retry_mask    = is_error | is_empty | is_no_emotion\n",
    "# retry_idx     = list(manifest[retry_mask].index)\n",
    "\n",
    "# print(f\"Empty          : {is_empty.sum()}\")\n",
    "# print(f\"Errored        : {is_error.sum()}\")\n",
    "# print(f\"No valid emotion: {is_no_emotion.sum()}\")\n",
    "# print(f\"Total to retry : {len(retry_idx)}\")\n",
    "\n",
    "# processed = 0\n",
    "\n",
    "# with tqdm(total=len(retry_idx), desc=\"Retry PaliGemma\") as pbar:\n",
    "#     for idx in retry_idx:\n",
    "#         row = manifest.loc[idx]\n",
    "#         video_path = str(BASE_DIR / row[\"path\"])\n",
    "\n",
    "#         try:\n",
    "#             result = predict_emotion_video(video_path, n_frames=PG_N_FRAMES)\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = result\n",
    "#         except Exception as e:\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = f\"ERROR: {e}\"\n",
    "#             print(f\"\\nError on {row['utterance_id']}: {e}\")\n",
    "\n",
    "#         processed += 1\n",
    "#         pbar.update(1)\n",
    "\n",
    "#         if processed % PG_SAVE_EVERY == 0:\n",
    "#             manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# # Final save\n",
    "# manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# total        = len(manifest)\n",
    "# done         = manifest[\"paligemma_emotion\"].notna().sum()\n",
    "# errors       = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "# still_no_emo = manifest[\"paligemma_emotion\"].map(_has_no_valid_emotion).sum()\n",
    "# print(f\"\\nDone. {done}/{total} clips processed.\")\n",
    "# print(f\"  Errors remaining        : {errors}\")\n",
    "# print(f\"  Still no valid emotion  : {still_no_emo}\")\n",
    "# print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "# manifest[retry_mask][[\"utterance_id\", \"emotion\", \"paligemma_emotion\"]].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
