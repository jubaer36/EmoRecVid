{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6434ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for VidEmo-7B (Qwen2.5-VL backbone)\n",
    "# %pip install -q qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a95221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "VRAM: 12.5 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "BASE_DIR = Path(\"/mnt/Work/ML/Code/EmoRecVid\")\n",
    "\n",
    "print(\"Imports OK\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137e2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: KlingTeam/VidEmo-3B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6af5bc76f04d348e22284ad7435b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model device map: {'model.visual': 0, 'model.language_model.embed_tokens': 0, 'lm_head': 0, 'model.language_model.layers.0': 0, 'model.language_model.layers.1': 0, 'model.language_model.layers.2': 0, 'model.language_model.layers.3': 0, 'model.language_model.layers.4': 0, 'model.language_model.layers.5': 0, 'model.language_model.layers.6': 0, 'model.language_model.layers.7': 0, 'model.language_model.layers.8': 0, 'model.language_model.layers.9': 0, 'model.language_model.layers.10': 0, 'model.language_model.layers.11': 'cpu', 'model.language_model.layers.12': 'cpu', 'model.language_model.layers.13': 'cpu', 'model.language_model.layers.14': 'cpu', 'model.language_model.layers.15': 'cpu', 'model.language_model.layers.16': 'cpu', 'model.language_model.layers.17': 'cpu', 'model.language_model.layers.18': 'cpu', 'model.language_model.layers.19': 'cpu', 'model.language_model.layers.20': 'cpu', 'model.language_model.layers.21': 'cpu', 'model.language_model.layers.22': 'cpu', 'model.language_model.layers.23': 'cpu', 'model.language_model.layers.24': 'cpu', 'model.language_model.layers.25': 'cpu', 'model.language_model.layers.26': 'cpu', 'model.language_model.layers.27': 'cpu', 'model.language_model.layers.28': 'cpu', 'model.language_model.layers.29': 'cpu', 'model.language_model.layers.30': 'cpu', 'model.language_model.layers.31': 'cpu', 'model.language_model.layers.32': 'cpu', 'model.language_model.layers.33': 'cpu', 'model.language_model.layers.34': 'cpu', 'model.language_model.layers.35': 'cpu', 'model.language_model.norm': 'cpu', 'model.language_model.rotary_emb': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"KlingTeam/VidEmo-3B\"\n",
    "PROCESSOR_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"   # base model that ships the processor config\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",   # remove if flash-attn not installed\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(PROCESSOR_ID)\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model device map: {model.hf_device_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37ad47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: /mnt/Work/ML/Code/EmoRecVid/utterance_clips/Session1/Ses01F_impro01/Ses01F_impro01_F000.avi\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using torchvision to read video.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video features a young adult woman with dark hair styled in a ponytail, sitting against a plain background. She is wearing a dark sleeveless top over a reddish-brown garment, complemented by a dark vest or wrap. A watch is visible on her left wrist, and she wears a small, dark hair accessory. Her expression is neutral, perhaps slightly pensive, as she gazes off-screen to her right. Her mouth remains mostly closed, with subtle movements suggesting she is speaking softly or listening intently. Her head is tilted slightly to the right, maintaining a consistent posture throughout the short clip. The lighting is dim, casting shadows that obscure some details of her face, making it difficult to discern finer facial features like eye color or skin tone. The overall impression is one of quiet contemplation or focused attention on something outside the frame.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_clip(video_path: str, max_new_tokens: int = 512, fps: float = 2.0) -> str:\n",
    "    \"\"\"\n",
    "    Run VidEmo-3B on a single utterance video clip and return a detailed\n",
    "    behavioral analysis (facial expressions, body language, behavioral cues).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": fps,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Watch this short video clip of a person speaking and provide a detailed analysis covering:\\n\"\n",
    "                        \"1. **Facial Expressions**: Describe the movements and cues observed (e.g., brow furrowing, lip tension, eye widening, smile, grimace).\\n\"\n",
    "                        \"2. **Body Language**: Describe posture, gestures, head movements, and any notable physical cues.\\n\"\n",
    "                        \"3. **Behavioral Cues**: Note speech rate changes, pauses, energy level, and any other observable behavioral signals.\\n\"\n",
    "                        \"4. **Overall Emotional State**: Summarise what emotion(s) are most likely being expressed and why.\\n\"\n",
    "                        \"Be specific and descriptive.\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# ── Analyse a single sample ───────────────────────────────────────────\n",
    "SAMPLE_VIDEO = str(BASE_DIR / \"utterance_clips/Session1/Ses01F_impro01/Ses01F_impro01_F000.avi\")\n",
    "\n",
    "print(f\"Video: {SAMPLE_VIDEO}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis = analyze_clip(SAMPLE_VIDEO)\n",
    "print(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb63fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
