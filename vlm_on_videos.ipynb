{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6434ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for Tarsier2-Recap-7b (Qwen2-VL backbone)\n",
    "# %pip install -q qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a95221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 06:46:01.928085: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-28 06:46:02.164913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "VRAM: 12.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "BASE_DIR = Path(\"/mnt/Work/ML/Code/EmoRecVid\")\n",
    "MANIFEST_PATH = BASE_DIR / \"utterance_clips\" / \"manifest.csv\"\n",
    "\n",
    "print(\"Imports OK\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: omni-research/Tarsier2-Recap-7b\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"omni-research/Tarsier2-Recap-7b\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # local_files_only=True,\n",
    "    # attn_implementation=\"flash_attention_2\",   # remove if flash-attn not installed\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, local_files_only=True)\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model device map: {model.hf_device_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ad47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_clip() ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_clip(video_path: str, max_new_tokens: int = 512, fps: float = 2.0) -> str:\n",
    "    \"\"\"\n",
    "    Run VidEmo-3B on a single utterance video clip and return a detailed\n",
    "    behavioral analysis (facial expressions, body language, behavioral cues).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": fps,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Watch this short video clip of a person speaking and provide a detailed analysis covering:\\n\"\n",
    "                        \"1. **Facial Expressions**: Describe the movements and cues observed (e.g., brow furrowing, lip tension, eye widening, smile, grimace).\\n\"\n",
    "                        \"2. **Body Language**: Describe posture, gestures, head movements, and any notable physical cues.\\n\"\n",
    "                        \"3. **Behavioral Cues**: Note speech rate changes, pauses, energy level, and any other observable behavioral signals.\\n\"\n",
    "                        \"4. **Overall Emotional State**: Summarise what emotion(s) are most likely being expressed and why.\\n\"\n",
    "                        \"Be specific and descriptive.\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"analyze_clip() ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb63fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips : 10039\n",
      "Already done: 9937  |  Remaining: 102\n",
      "Queued for processing: 102 clips\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4a0a3df5b34f32aeb3c08128edc2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VLM analysis:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. 10039/10039 clips processed, 0 errors.\n",
      "Results saved → /mnt/Work/ML/Code/EmoRecVid/utterance_clips/manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>crop_side</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>path</th>\n",
       "      <th>vlm_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>6.2901</td>\n",
       "      <td>8.2357</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The video features a young adult woman with da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>11.3925</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The video features a young adult woman with da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>14.8872</td>\n",
       "      <td>18.0175</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The video features a young adult female with s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          utterance_id emotion speaker_gender crop_side    start      end  \\\n",
       "0  Ses01F_impro01_F000     neu              F      left   6.2901   8.2357   \n",
       "1  Ses01F_impro01_F001     neu              F      left  10.0100  11.3925   \n",
       "2  Ses01F_impro01_F002     neu              F      left  14.8872  18.0175   \n",
       "\n",
       "                                                path  \\\n",
       "0  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "1  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "2  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "\n",
       "                                        vlm_analysis  \n",
       "0  The video features a young adult woman with da...  \n",
       "1  The video features a young adult woman with da...  \n",
       "2  The video features a young adult female with s...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load manifest – preserve any already-completed analyses\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "total  = len(manifest)\n",
    "done_before = manifest[\"vlm_analysis\"].notna().sum()\n",
    "print(f\"Total clips : {total}\")\n",
    "print(f\"Already done: {done_before}  |  Remaining: {total - done_before}\")\n",
    "\n",
    "# ── Batch inference helper ────────────────────────────────────────────\n",
    "BATCH_SIZE = 8      # increase for throughput (reduce if OOM)\n",
    "SAVE_EVERY = 20     # checkpoint to disk every N clips\n",
    "FPS        = 1.0    # frames-per-second fed to the model (fewer frames = faster)\n",
    "\n",
    "\n",
    "def analyze_batch(video_paths: list[str], max_new_tokens: int = 256, fps: float = FPS) -> list[str]:\n",
    "    \"\"\"Run the VLM on a batch of video clips and return one response per clip.\"\"\"\n",
    "    PROMPT_TEXT = (\n",
    "        \"Watch this short video clip of a person speaking and provide a detailed analysis covering:\\n\"\n",
    "        \"1. **Facial Expressions**: Describe the movements and cues observed (e.g., brow furrowing, lip tension, eye widening, smile, grimace).\\n\"\n",
    "        \"2. **Body Language**: Describe posture, gestures, head movements, and any notable physical cues.\\n\"\n",
    "        \"3. **Behavioral Cues**: Note speech rate changes, pauses, energy level, and any other observable behavioral signals.\\n\"\n",
    "        \"4. **Overall Emotional State**: Summarise what emotion(s) are most likely being expressed and why.\\n\"\n",
    "        \"Be specific and descriptive.\"\n",
    "    )\n",
    "\n",
    "    all_messages = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": vp, \"fps\": fps, \"max_pixels\": 360 * 420},\n",
    "                    {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for vp in video_paths\n",
    "    ]\n",
    "\n",
    "    texts = [\n",
    "        processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        for msgs in all_messages\n",
    "    ]\n",
    "\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "        all_messages, return_video_kwargs=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    responses = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return [r.strip() for r in responses]\n",
    "\n",
    "\n",
    "# ── Main loop ─────────────────────────────────────────────────────────\n",
    "# Only process rows that have not been analysed yet (NaN) or previously errored\n",
    "pending = list(manifest[manifest[\"vlm_analysis\"].isna()].index)\n",
    "print(f\"Queued for processing: {len(pending)} clips\\n\")\n",
    "\n",
    "processed = 0\n",
    "\n",
    "with tqdm(total=len(pending), desc=\"VLM analysis\") as pbar:\n",
    "    for batch_start in range(0, len(pending), BATCH_SIZE):\n",
    "        batch_idx = pending[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "        video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "        try:\n",
    "            results = analyze_batch(video_paths)\n",
    "            for idx, analysis in zip(batch_idx, results):\n",
    "                manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "        except Exception as e:\n",
    "            for idx in batch_idx:\n",
    "                manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "            print(f\"\\nError on batch {batch_idx}: {e}\")\n",
    "\n",
    "        processed += len(batch_idx)\n",
    "        pbar.update(len(batch_idx))\n",
    "\n",
    "        # Periodic checkpoint\n",
    "        if processed % SAVE_EVERY == 0:\n",
    "            manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final save\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "done   = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nDone. {done}/{total} clips processed, {errors} errors.\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "manifest.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b965e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty  : 0\n",
      "Errored: 0\n",
      "Total to retry: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb6510dfb6d403ca89166c3b43fc985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retry VLM analysis: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. 10039/10039 clips processed, 0 errors remaining.\n",
      "Results saved → /mnt/Work/ML/Code/EmoRecVid/utterance_clips/manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>crop_side</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>path</th>\n",
       "      <th>vlm_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [utterance_id, emotion, speaker_gender, crop_side, start, end, path, vlm_analysis]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ── Retry empty / errored clips ───────────────────────────────────────\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "is_error = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False)\n",
    "is_empty = manifest[\"vlm_analysis\"].isna()\n",
    "retry_mask = is_error | is_empty\n",
    "\n",
    "retry_idx = list(manifest[retry_mask].index)\n",
    "print(f\"Empty  : {is_empty.sum()}\")\n",
    "print(f\"Errored: {is_error.sum()}\")\n",
    "print(f\"Total to retry: {len(retry_idx)}\")\n",
    "\n",
    "processed = 0\n",
    "\n",
    "with tqdm(total=len(retry_idx), desc=\"Retry VLM analysis\") as pbar:\n",
    "    for batch_start in range(0, len(retry_idx), BATCH_SIZE):\n",
    "        batch_idx = retry_idx[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "        video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "        try:\n",
    "            results = analyze_batch(video_paths)\n",
    "            for idx, analysis in zip(batch_idx, results):\n",
    "                manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "        except Exception as e:\n",
    "            for idx in batch_idx:\n",
    "                manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "            print(f\"\\nError on batch {batch_idx}: {e}\")\n",
    "\n",
    "        processed += len(batch_idx)\n",
    "        pbar.update(len(batch_idx))\n",
    "\n",
    "        if processed % SAVE_EVERY == 0:\n",
    "            manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final save\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "total   = len(manifest)\n",
    "done    = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors  = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nDone. {done}/{total} clips processed, {errors} errors remaining.\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "manifest[retry_mask].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4840d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using torchvision to read video.\n",
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The video features a young adult woman with shoulder-length dark brown hair styled in a low ponytail, sitting against a plain off-white wall. She wears a black headband adorned with a small white flower and a dark bracelet on her left wrist. Her face is round, with a rounded chin and narrow lips. Initially, she appears pensive or neutral, with her gaze directed downwards towards her lap. Her mouth corners are slightly downturned, and her eyebrows remain neutral. She fidgets with her hands, suggesting a degree of restlessness or preoccupation. As she begins to speak, her mouth moves, and her teeth become briefly visible. Her gaze shifts upwards momentarily before returning to its downward position. Her expression remains largely unchanged, though there's a subtle shift towards a more neutral demeanor as she continues talking. Her head remains relatively still throughout, with only slight tilts and nods accompanying her speech. The lighting is dim, casting subtle shadows on her face, particularly around her eyes. The overall impression is one of quiet contemplation or perhaps mild concern, conveyed through her downward gaze, minimal facial expressions, and subtle hand movements. The scene remains static, with no camera movement or changes in background.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze_clip(\"utterance_clips/Session1/Ses01F_impro06/Ses01F_impro06_F006.avi\", max_new_tokens=512, fps=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
