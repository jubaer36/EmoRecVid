{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc6434ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies for Tarsier2-Recap-7b (Qwen2-VL backbone)\n",
    "# %pip install -q qwen-vl-utils\n",
    "\n",
    "# Install missing dependencies for FaceScanPaliGemma_Emotion\n",
    "# %pip install -q transformers>=4.42.0 Pillow opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a95221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "VRAM: 12.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MANIFEST_PATH = BASE_DIR / \"utterance_clips\" / \"manifest.csv\"\n",
    "\n",
    "print(\"Imports OK\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: KlingTeam/VidEmo-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Work/Environments/Ubuntu/Conda/envs/kaggle/lib/python3.12/site-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 4776 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9263e9a5920a4d8d8a15a14545cb021d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_ID = \"KlingTeam/VidEmo-7B\"\n",
    "PROCESSOR_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\"   # base model that ships the processor config\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",   # remove if flash-attn not installed\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(PROCESSOR_ID)\n",
    "# Decoder-only models require left-padding for correct batched generation\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ad47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_clip() ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROMPT_TEXT = (\n",
    "    \"Watch this short video clip of a person speaking and provide a detailed analysis covering:\\n\"\n",
    "                        \"1. **Facial Expressions**: Describe the movements and cues observed (e.g., brow furrowing, lip tension, eye widening, smile, grimace).\\n\"\n",
    "                        \"2. **Body Language**: Describe posture, gestures, head movements, and any notable physical cues.\\n\"\n",
    "                        \"3. **Behavioral Cues**: Note speech rate changes, pauses, energy level, and any other observable behavioral signals.\\n\"\n",
    "                        \"4. **Overall Emotional State**: Summarise what emotion(s) are most likely being expressed and why.\\n\"\n",
    "                        \"Be specific and descriptive.\"\n",
    ")\n",
    "\n",
    "\n",
    "def analyze_clip(video_path: str, max_new_tokens: int = 512, fps: float = 2.0) -> str:\n",
    "    \"\"\"\n",
    "    Run VidEmo-3B on a single utterance video clip and return a temporal\n",
    "    emotional arc analysis (how expressions change from start to end).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": fps,\n",
    "                    \"max_pixels\": 360 * 420,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"analyze_clip() ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64347f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load manifest – preserve any already-completed analyses\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "manifest[\"vlm_analysis\"] = manifest[\"vlm_analysis\"].astype(\"object\")\n",
    "# Create column if it doesn't exist\n",
    "if \"vlm_analysis\" not in manifest.columns:\n",
    "    manifest[\"vlm_analysis\"] = pd.NA\n",
    "manifest['vlm_analysis'] = None\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb63fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips : 10039\n",
      "Already done: 10039  |  Remaining: 0\n",
      "Queued for processing: 0 clips\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6224def50494382bb3948159f857c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VLM analysis: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. 10039/10039 clips processed, 0 errors.\n",
      "Results saved → /mnt/Work/ML/Code/EmoRecVid/utterance_clips/manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>crop_side</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>path</th>\n",
       "      <th>vlm_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>6.2901</td>\n",
       "      <td>8.2357</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>11.3925</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>14.8872</td>\n",
       "      <td>18.0175</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          utterance_id emotion speaker_gender crop_side    start      end  \\\n",
       "0  Ses01F_impro01_F000     neu              F      left   6.2901   8.2357   \n",
       "1  Ses01F_impro01_F001     neu              F      left  10.0100  11.3925   \n",
       "2  Ses01F_impro01_F002     neu              F      left  14.8872  18.0175   \n",
       "\n",
       "                                                path  \\\n",
       "0  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "1  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "2  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "\n",
       "                                        vlm_analysis  \n",
       "0  The woman begins with a neutral expression, he...  \n",
       "1  The woman begins with a neutral expression, he...  \n",
       "2  The woman begins with a neutral expression, he...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "total  = len(manifest)\n",
    "done_before = manifest[\"vlm_analysis\"].notna().sum()\n",
    "print(f\"Total clips : {total}\")\n",
    "print(f\"Already done: {done_before}  |  Remaining: {total - done_before}\")\n",
    "\n",
    "# ── Config ────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE = 8      # increase for throughput (reduce if OOM)\n",
    "SAVE_EVERY = 20     # checkpoint to disk every N clips\n",
    "FPS        = 1.0    # higher fps = more temporal resolution for tracking changes\n",
    "\n",
    "\n",
    "def analyze_batch(video_paths: list[str], max_new_tokens: int = 512, fps: float = FPS) -> list[str]:\n",
    "    \"\"\"Run the VLM on a batch of video clips and return a temporal arc analysis per clip.\"\"\"\n",
    "    PROMPT_TEXT = (\n",
    "        \"Watch this short video clip of a person speaking and describe how their emotional state evolves over time.\\n\"\n",
    "        \"Structure your response as a temporal progression — divide the clip into beginning, middle, and end (or more segments if the emotion changes more than once).\\n\"\n",
    "        \"For each segment describe:\\n\"\n",
    "        \"- **Facial Expressions**: Specific muscle movements (brow, lips, eyes, jaw, cheeks).\\n\"\n",
    "        \"- **Head & Gaze**: Tilts, nods, shakes, eye direction.\\n\"\n",
    "        \"- **Body Language**: Posture shifts, gestures, tension or relaxation.\\n\"\n",
    "        \"- **Emotion at this moment**: The most likely emotion and the visual cues supporting it.\\n\"\n",
    "        \"Finish with a one-sentence summary of the overall emotional arc (e.g., starts neutral → builds frustration → brief smile at end).\\n\"\n",
    "        \"Ground every observation in a specific visible signal.\"\n",
    "    )\n",
    "\n",
    "    all_messages = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": vp, \"fps\": fps, \"max_pixels\": 360 * 420},\n",
    "                    {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for vp in video_paths\n",
    "    ]\n",
    "\n",
    "    texts = [\n",
    "        processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        for msgs in all_messages\n",
    "    ]\n",
    "\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "        all_messages, return_video_kwargs=True\n",
    "    )\n",
    "\n",
    "    if \"fps\" in video_kwargs and isinstance(video_kwargs[\"fps\"], list):\n",
    "        del video_kwargs[\"fps\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        **video_kwargs,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    responses = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return [r.strip() for r in responses]\n",
    "\n",
    "\n",
    "# ── Main loop ─────────────────────────────────────────────────────────\n",
    "pending = list(manifest[manifest[\"vlm_analysis\"].isna()].index)\n",
    "print(f\"Queued for processing: {len(pending)} clips\\n\")\n",
    "\n",
    "processed = 0\n",
    "\n",
    "with tqdm(total=len(pending), desc=\"VLM analysis\") as pbar:\n",
    "    for batch_start in range(0, len(pending), BATCH_SIZE):\n",
    "        batch_idx = pending[batch_start : batch_start + BATCH_SIZE]\n",
    "        batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "        video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "        try:\n",
    "            results = analyze_batch(video_paths)\n",
    "            for idx, analysis in zip(batch_idx, results):\n",
    "                manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "        except Exception as e:\n",
    "            for idx in batch_idx:\n",
    "                manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "            print(f\"\\nError on batch {batch_idx}: {e}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        prev_processed = processed\n",
    "        processed += len(batch_idx)\n",
    "        pbar.update(len(batch_idx))\n",
    "\n",
    "        if processed // SAVE_EVERY > prev_processed // SAVE_EVERY:\n",
    "            manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final save\n",
    "manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "done   = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nDone. {done}/{total} clips processed, {errors} errors.\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "manifest.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b965e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Pass 0 ── Empty: 0  |  Errored: 0  |  Bad: 0  |  Total to fix: 0\n",
      "All clips are clean — done!\n",
      "\n",
      "Final: 10039/10039 clips processed, 0 errors, 0 retry pass(es).\n",
      "Results saved → /mnt/Work/ML/Code/EmoRecVid/utterance_clips/manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker_gender</th>\n",
       "      <th>crop_side</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>path</th>\n",
       "      <th>vlm_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>6.2901</td>\n",
       "      <td>8.2357</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>11.3925</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>neu</td>\n",
       "      <td>F</td>\n",
       "      <td>left</td>\n",
       "      <td>14.8872</td>\n",
       "      <td>18.0175</td>\n",
       "      <td>utterance_clips/Session1/Ses01F_impro01/Ses01F...</td>\n",
       "      <td>The woman begins with a neutral expression, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          utterance_id emotion speaker_gender crop_side    start      end  \\\n",
       "0  Ses01F_impro01_F000     neu              F      left   6.2901   8.2357   \n",
       "1  Ses01F_impro01_F001     neu              F      left  10.0100  11.3925   \n",
       "2  Ses01F_impro01_F002     neu              F      left  14.8872  18.0175   \n",
       "\n",
       "                                                path  \\\n",
       "0  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "1  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "2  utterance_clips/Session1/Ses01F_impro01/Ses01F...   \n",
       "\n",
       "                                        vlm_analysis  \n",
       "0  The woman begins with a neutral expression, he...  \n",
       "1  The woman begins with a neutral expression, he...  \n",
       "2  The woman begins with a neutral expression, he...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ── Retry empty / errored / bad-output clips (loops until all are clean) ──\n",
    "manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "def _is_bad_output(val) -> bool:\n",
    "    \"\"\"True if a non-error value looks like a corrupted/truncated generation.\"\"\"\n",
    "    if pd.isna(val) or str(val).startswith(\"ERROR:\"):\n",
    "        return False\n",
    "    s = str(val).strip()\n",
    "    words = s.split()\n",
    "    if len(words) <= 10:\n",
    "        return True\n",
    "    if words[0].lower() == \"the\" and words[-1].lower() == \"the\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_retry_mask(df):\n",
    "    is_error = df[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False)\n",
    "    is_empty = df[\"vlm_analysis\"].isna()\n",
    "    is_bad   = df[\"vlm_analysis\"].map(_is_bad_output)\n",
    "    return is_error | is_empty | is_bad\n",
    "\n",
    "pass_num = 0\n",
    "\n",
    "while True:\n",
    "    retry_mask = get_retry_mask(manifest)\n",
    "    retry_idx  = list(manifest[retry_mask].index)\n",
    "\n",
    "    is_error = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False)\n",
    "    is_empty = manifest[\"vlm_analysis\"].isna()\n",
    "    is_bad   = manifest[\"vlm_analysis\"].map(_is_bad_output)\n",
    "\n",
    "    print(f\"\\n── Pass {pass_num} ── Empty: {is_empty.sum()}  |  Errored: {is_error.sum()}  |  Bad: {is_bad.sum()}  |  Total to fix: {len(retry_idx)}\")\n",
    "\n",
    "    if len(retry_idx) == 0:\n",
    "        print(\"All clips are clean — done!\")\n",
    "        break\n",
    "\n",
    "    pass_num += 1\n",
    "    processed = 0\n",
    "\n",
    "    with tqdm(total=len(retry_idx), desc=f\"Retry pass {pass_num}\") as pbar:\n",
    "        for batch_start in range(0, len(retry_idx), BATCH_SIZE):\n",
    "            batch_idx  = retry_idx[batch_start : batch_start + BATCH_SIZE]\n",
    "            batch_rows = manifest.loc[batch_idx]\n",
    "\n",
    "            video_paths = [str(BASE_DIR / row[\"path\"]) for _, row in batch_rows.iterrows()]\n",
    "\n",
    "            try:\n",
    "                results = analyze_batch(video_paths)\n",
    "                for idx, analysis in zip(batch_idx, results):\n",
    "                    manifest.at[idx, \"vlm_analysis\"] = analysis\n",
    "            except Exception as e:\n",
    "                for idx in batch_idx:\n",
    "                    manifest.at[idx, \"vlm_analysis\"] = f\"ERROR: {e}\"\n",
    "                print(f\"\\nError on batch {list(batch_idx)}: {e}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            prev_processed = processed\n",
    "            processed += len(batch_idx)\n",
    "            pbar.update(len(batch_idx))\n",
    "\n",
    "            if processed // SAVE_EVERY > prev_processed // SAVE_EVERY:\n",
    "                manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "    manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# Final summary\n",
    "total  = len(manifest)\n",
    "done   = manifest[\"vlm_analysis\"].notna().sum()\n",
    "errors = manifest[\"vlm_analysis\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "print(f\"\\nFinal: {done}/{total} clips processed, {errors} errors, {pass_num} retry pass(es).\")\n",
    "print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "\n",
    "manifest.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4840d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_clip(\"utterance_clips/Session1/Ses01F_impro06/Ses01F_impro06_F006.avi\", max_new_tokens=512, fps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e5e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── FaceScanPaliGemma_Emotion ─────────────────────────────────────────\n",
    "# # Fine-tuned PaliGemma-3B on AffectNet; classifies a face image into one of:\n",
    "# #   neutral | happy | sad | surprise | fear | disgust | anger | contempt\n",
    "# # Model card: https://huggingface.co/NYUAD-ComNets/FaceScanPaliGemma_Emotion\n",
    "\n",
    "# import os\n",
    "# # Disable the xet/CAS backend so HF falls back to standard HTTP downloads\n",
    "# os.environ[\"HF_HUB_DISABLE_XET_BACKEND\"] = \"1\"\n",
    "\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# from tqdm.auto import tqdm\n",
    "# from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "# from collections import Counter\n",
    "\n",
    "# BASE_DIR = Path(\"/mnt/Work/ML/Code/EmoRecVid\")\n",
    "# MANIFEST_PATH = BASE_DIR / \"utterance_clips\" / \"manifest.csv\"\n",
    "\n",
    "# PG_MODEL_ID = \"NYUAD-ComNets/FaceScanPaliGemma_Emotion\"\n",
    "# PG_PROC_ID  = \"google/paligemma-3b-pt-224\"\n",
    "# PG_PROMPT   = \"what is the emotion of the person in the image?\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Device: {device}\")\n",
    "\n",
    "# print(f\"Loading processor from {PG_PROC_ID} …\")\n",
    "# pg_processor = PaliGemmaProcessor.from_pretrained(PG_PROC_ID)\n",
    "\n",
    "# print(f\"Loading model from {PG_MODEL_ID} …\")\n",
    "# pg_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "#     PG_MODEL_ID,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# pg_model.to(device)\n",
    "# pg_model.eval()\n",
    "# print(\"PaliGemma emotion model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "360932f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VALID_EMOTIONS = {\"neutral\", \"happy\", \"sad\", \"surprise\", \"fear\", \"disgust\", \"anger\", \"contempt\"}\n",
    "\n",
    "# def extract_frames(video_path: str, n_frames: int = 5) -> list[Image.Image]:\n",
    "#     \"\"\"Extract `n_frames` evenly-spaced frames from a video file as PIL images.\"\"\"\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     if total == 0:\n",
    "#         cap.release()\n",
    "#         return []\n",
    "#     indices = np.linspace(0, total - 1, min(n_frames, total), dtype=int)\n",
    "#     frames = []\n",
    "#     for idx in indices:\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "#         ret, frame = cap.read()\n",
    "#         if ret:\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frames.append(Image.fromarray(frame_rgb))\n",
    "#     cap.release()\n",
    "#     return frames\n",
    "\n",
    "\n",
    "# # PaliGemma requires the <image> token at the start of the prompt\n",
    "# _PG_PROMPT_WITH_TOKEN = \"<image> \" + PG_PROMPT\n",
    "\n",
    "# def predict_emotion_video(video_path: str, n_frames: int = 5) -> str:\n",
    "#     \"\"\"\n",
    "#     Extract frames, run PaliGemma on each frame, and return the per-frame\n",
    "#     emotion labels as a space-separated string, e.g.:\n",
    "#         \"neutral happy neutral sad neutral\"\n",
    "#     Labels that fall outside VALID_EMOTIONS are included as-is so no\n",
    "#     information is lost.\n",
    "#     Returns \"ERROR: no frames\" if the video has no readable frames.\n",
    "#     \"\"\"\n",
    "#     frames = extract_frames(video_path, n_frames=n_frames)\n",
    "#     if not frames:\n",
    "#         return \"ERROR: no frames\"\n",
    "\n",
    "#     per_frame_labels = []\n",
    "\n",
    "#     for frame in frames:\n",
    "#         inputs = pg_processor(\n",
    "#             text=_PG_PROMPT_WITH_TOKEN,\n",
    "#             images=frame,\n",
    "#             padding=\"longest\",\n",
    "#             do_convert_rgb=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(device)\n",
    "#         inputs = inputs.to(dtype=pg_model.dtype)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = pg_model.generate(**inputs, max_length=500)\n",
    "\n",
    "#         # Trim input tokens, decode only newly generated tokens\n",
    "#         new_tokens = output[0][inputs.input_ids.shape[1]:]\n",
    "#         decoded = pg_processor.decode(new_tokens, skip_special_tokens=True).strip().lower()\n",
    "\n",
    "#         # First word as the label candidate\n",
    "#         label = decoded.split()[0] if decoded.split() else \"unknown\"\n",
    "#         per_frame_labels.append(label)\n",
    "\n",
    "#     return \" \".join(per_frame_labels)\n",
    "\n",
    "\n",
    "# print(\"predict_emotion_video() ready  (returns space-separated per-frame labels)\")\n",
    "# print(f\"Prompt: {_PG_PROMPT_WITH_TOKEN!r}\")\n",
    "# print(f\"Extracts 5 frames per clip – one label per frame, all stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f365b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Quick sanity-check: sample a random clip and visualise predictions ─\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display\n",
    "\n",
    "# manifest_preview = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# # Pick a random row\n",
    "# sample_row = manifest_preview.sample(1).iloc[0]\n",
    "# sample_path = str(BASE_DIR / sample_row[\"path\"])\n",
    "\n",
    "# print(f\"Utterance : {sample_row['utterance_id']}\")\n",
    "# print(f\"GT emotion: {sample_row['emotion']}\")\n",
    "# print(f\"Video path: {sample_path}\")\n",
    "# print()\n",
    "\n",
    "# # Run through predict_emotion_video – returns \"label1 label2 label3 ...\"\n",
    "# result_str = predict_emotion_video(sample_path, n_frames=5)\n",
    "# per_frame_labels = result_str.split()\n",
    "# valid = [l for l in per_frame_labels if l in VALID_EMOTIONS]\n",
    "\n",
    "# print(f\"Per-frame labels : {per_frame_labels}\")\n",
    "# print(f\"Valid labels     : {valid}\")\n",
    "# print(f\"Dominant         : {Counter(valid).most_common(1)[0][0] if valid else 'none'}\")\n",
    "\n",
    "# # Plot frames with their labels\n",
    "# frames = extract_frames(sample_path, n_frames=5)\n",
    "# n = len(frames)\n",
    "# fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "# if n == 1:\n",
    "#     axes = [axes]\n",
    "\n",
    "# for ax, frame, lbl in zip(axes, frames, per_frame_labels):\n",
    "#     ax.imshow(frame)\n",
    "#     color = \"green\" if lbl in VALID_EMOTIONS else \"red\"\n",
    "#     ax.set_title(lbl, fontsize=13, color=color, fontweight=\"bold\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# dominant = Counter(valid).most_common(1)[0][0] if valid else \"none\"\n",
    "# fig.suptitle(\n",
    "#     f\"GT: {sample_row['emotion']}   |   Labels: {' '.join(per_frame_labels)}   |   Dominant: {dominant}\",\n",
    "#     fontsize=12, fontweight=\"bold\", y=1.02\n",
    "# )\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "230b4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Main inference loop ───────────────────────────────────────────────\n",
    "# manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# # Ensure the column exists\n",
    "# if \"paligemma_emotion\" not in manifest.columns:\n",
    "#     manifest[\"paligemma_emotion\"] = pd.NA\n",
    "\n",
    "# # Reset all existing predictions so everything gets reprocessed\n",
    "# manifest[\"paligemma_emotion\"] = pd.NA\n",
    "\n",
    "# total   = len(manifest)\n",
    "# pending = list(manifest.index)\n",
    "\n",
    "# print(f\"Total clips to process: {total}\")\n",
    "# print(\"Output format: space-separated per-frame labels, e.g. 'neutral happy neutral sad neutral'\")\n",
    "\n",
    "# PG_N_FRAMES   = 5     # frames sampled per clip\n",
    "# PG_SAVE_EVERY = 50    # checkpoint frequency\n",
    "\n",
    "# processed = 0\n",
    "\n",
    "# with tqdm(total=len(pending), desc=\"PaliGemma emotion\") as pbar:\n",
    "#     for idx in pending:\n",
    "#         row = manifest.loc[idx]\n",
    "#         video_path = str(BASE_DIR / row[\"path\"])\n",
    "\n",
    "#         try:\n",
    "#             result = predict_emotion_video(video_path, n_frames=PG_N_FRAMES)\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = result\n",
    "#         except Exception as e:\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = f\"ERROR: {e}\"\n",
    "#             print(f\"\\nError on {row['utterance_id']}: {e}\")\n",
    "\n",
    "#         processed += 1\n",
    "#         pbar.update(1)\n",
    "\n",
    "#         if processed % PG_SAVE_EVERY == 0:\n",
    "#             manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# # Final save\n",
    "# manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# done   = manifest[\"paligemma_emotion\"].notna().sum()\n",
    "# errors = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "# print(f\"\\nDone. {done}/{total} clips processed, {errors} errors.\")\n",
    "# print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "# manifest[[\"utterance_id\", \"emotion\", \"paligemma_emotion\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "333ce973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ── Retry errored / empty / no-valid-emotion PaliGemma predictions ────\n",
    "# manifest = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# def _has_no_valid_emotion(val) -> bool:\n",
    "#     \"\"\"True if val is a non-error string but contains no recognised emotion word.\"\"\"\n",
    "#     if pd.isna(val) or str(val).startswith(\"ERROR:\"):\n",
    "#         return False\n",
    "#     return not any(w in VALID_EMOTIONS for w in str(val).split())\n",
    "\n",
    "# is_error      = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False)\n",
    "# is_empty      = manifest[\"paligemma_emotion\"].isna()\n",
    "# is_no_emotion = manifest[\"paligemma_emotion\"].map(_has_no_valid_emotion)\n",
    "# retry_mask    = is_error | is_empty | is_no_emotion\n",
    "# retry_idx     = list(manifest[retry_mask].index)\n",
    "\n",
    "# print(f\"Empty          : {is_empty.sum()}\")\n",
    "# print(f\"Errored        : {is_error.sum()}\")\n",
    "# print(f\"No valid emotion: {is_no_emotion.sum()}\")\n",
    "# print(f\"Total to retry : {len(retry_idx)}\")\n",
    "\n",
    "# processed = 0\n",
    "\n",
    "# with tqdm(total=len(retry_idx), desc=\"Retry PaliGemma\") as pbar:\n",
    "#     for idx in retry_idx:\n",
    "#         row = manifest.loc[idx]\n",
    "#         video_path = str(BASE_DIR / row[\"path\"])\n",
    "\n",
    "#         try:\n",
    "#             result = predict_emotion_video(video_path, n_frames=PG_N_FRAMES)\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = result\n",
    "#         except Exception as e:\n",
    "#             manifest.at[idx, \"paligemma_emotion\"] = f\"ERROR: {e}\"\n",
    "#             print(f\"\\nError on {row['utterance_id']}: {e}\")\n",
    "\n",
    "#         processed += 1\n",
    "#         pbar.update(1)\n",
    "\n",
    "#         if processed % PG_SAVE_EVERY == 0:\n",
    "#             manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# # Final save\n",
    "# manifest.to_csv(MANIFEST_PATH, index=False)\n",
    "\n",
    "# total        = len(manifest)\n",
    "# done         = manifest[\"paligemma_emotion\"].notna().sum()\n",
    "# errors       = manifest[\"paligemma_emotion\"].str.startswith(\"ERROR:\", na=False).sum()\n",
    "# still_no_emo = manifest[\"paligemma_emotion\"].map(_has_no_valid_emotion).sum()\n",
    "# print(f\"\\nDone. {done}/{total} clips processed.\")\n",
    "# print(f\"  Errors remaining        : {errors}\")\n",
    "# print(f\"  Still no valid emotion  : {still_no_emo}\")\n",
    "# print(f\"Results saved → {MANIFEST_PATH}\")\n",
    "# manifest[retry_mask][[\"utterance_id\", \"emotion\", \"paligemma_emotion\"]].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
